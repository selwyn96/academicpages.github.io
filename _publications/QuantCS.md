---
title: "Sample Complexity Bounds for 1-bit Compressive Sensing and Binary Stable Embeddings with Generative Priors"
collection: publications
permalink: /publication/QuantCS
venue: "The 37th International Conference on Machine Learning (ICML 2020)"
date: 2020-06-01
[[PDF]](https://arxiv.org/abs/2002.01697)
citation: 'Zhaoqiang Liu, <b>Selwyn Gomes<b>, Avtansh Tiwari, Jonathan Scarlett.
<i>The 37th International Conference on Machine Learning</i>. <b>ICML 2020</b>.'
---
## Abstract
The goal of standard 1-bit compressive sensing is to accurately recover an unknown sparse vector from binary-valued measurements, each indicating the sign of a linear function of the vector. Motivated by recent advances in compressive sensing with generative models, where a generative modeling assumption replaces the usual sparsity assumption, we study the problem of 1-bit compressive sensing with generative models. We first consider noiseless 1-bit measurements, and provide sample complexity bounds for approximate recovery under i.i.d.~Gaussian measurements and a Lipschitz continuous generative prior, as well as a near-matching algorithm-independent lower bound. Moreover, we demonstrate that the Binary Ïµ-Stable Embedding property, which characterizes the robustness of the reconstruction to measurement errors and noise, also holds for 1-bit compressive sensing with Lipschitz continuous generative models with sufficiently many Gaussian measurements. In addition, we apply our results to neural network generative models, and provide a proof-of-concept numerical experiment demonstrating significant improvements over sparsity-based approaches.
